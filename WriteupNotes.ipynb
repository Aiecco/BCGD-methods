{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithims\n",
    "### Gradient Descent\n",
    "### BCGD\n",
    "What is the motivation of BCGD - I think it is exploiting convexity to make less work.  \n",
    "Random rule - does not require calculating the full gradent, but only one block at at time. This saves work at each iteration, but can waste time selecting blocks that have already been optimized.  \n",
    "Optimality conditions: check norm of gradient or block of gradient. if less than 0.01 stop. For random rule we use paticence, since selecting previously optimized blocks will have low gradient and would otherwise trigger stopping before convergence.  \n",
    "Formally state the steps of each algorithim.  \n",
    "Will probally discuss the results in the dataset sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-size\n",
    "The more theoretical formulations for block coordinate descent methods use line searching to find an optimal point in the direction of the current gradient block. This does well to minimize the number of iterations for convergence by exploiting the convexity of the logistic classification problem. [expand]\n",
    "Here we visualize the loss along a gradient block selected at an iteration of BCGD with GS rule with varying step sizes.\n",
    "[image]\n",
    "In practice computing he loss at many different points along this line to find the optimal step is a time expensive operation. Second order methods \n",
    "Calculating the Lipchitz constant... Requires hessian. This is expensive. I think this would need to be done at each iteration, too. Can use approximation of lipshitz constant to create fixed stepsize that works well enough. Maybe mention adaptive rates. Reference slides for writing this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntetic Dataset\n",
    "Gradient descent performs noticably better than both BCGD methods. We attribute this to the high-dimensionality of examples resulting in a large number of blocks that need to be individually optimized, where methods using the full gradient can decrease many of these at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
