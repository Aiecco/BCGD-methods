{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithims\n",
    "### Gradient Descent\n",
    "### BCGD\n",
    "What is the motivation of BCGD - I think it is exploiting convexity to make less work.  \n",
    "Random rule - does not require calculating the full gradent, but only one block at at time. This saves work at each iteration, but can waste time selecting blocks that have already been optimized.  \n",
    "Optimality conditions: check norm of gradient or block of gradient. if less than 0.01 stop. For random rule we use paticence, since selecting previously optimized blocks will have low gradient and would otherwise trigger stopping before convergence.  \n",
    "Formally state the steps of each algorithim.  \n",
    "Will probally discuss the results in the dataset sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-size\n",
    "The more theoretical formulations for block coordinate descent methods use line searching to find an optimal point in the direction of the current gradient block. This does well to minimize the number of iterations for convergence by exploiting the convexity of the logistic classification problem. [expand]\n",
    "Here we visualize the loss along a gradient block selected at an iteration of BCGD with GS rule with varying step sizes.\n",
    "[image]\n",
    "In practice computing he loss at many different points along this line to find the optimal step is a time expensive operation. Second order methods \n",
    "Calculating the Lipchitz constant... Requires hessian. This is expensive. I think this would need to be done at each iteration, too. Can use approximation of lipshitz constant to create fixed stepsize that works well enough. Maybe mention adaptive rates. Reference slides for writing this section.\n",
    "\n",
    "There are many approaches to approximating the Lipshitz constant. The major consideration is to balance the cost of estimating with estimation accuracy. This is of critical importance because The Lipshitz constant is dependent on the currant parameter matrix $X$, meaning the value of $L$ will change for each iteration, requiring recalculating. If this calculation is too costly, Block coordinate methods will loose any possible advantage they confer by having a less expensive gradient calculation, and too inpercise will cause instability and prevent the methods from converging.  \n",
    "As a starting point we can consider our initialization of $X$ to have entries drawn from $N(0,1)$. Assuming sucessive steps to be drawn from the same distribution, we can estimate $L$ emperically with by sampeling many $A$ and $B$ and calculating:\n",
    "$$L = \\max_{A,B} \\frac{||\\nabla_X A - \\nabla_X B||}{||A - B||}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntetic Dataset\n",
    "Gradient descent performs noticably better than both BCGD methods. We attribute this to the high-dimensionality of examples resulting in a large number of blocks that need to be individually optimized, where methods using the full gradient can decrease many of these at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
