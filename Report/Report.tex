\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{authblk}

\title{Gradient Methods for Muli-class Logistic Regression}
\author{Alessandro Pala - 2107800 \and 
Tanner Aaron Graves - 2073559\and 
Alisa Snezskaia - 2107497\and
Anna Glado - 2122285}

%\affil{Optimazation for DataScience, UniPD}
\date{May 2024}

\begin{document}


\maketitle

\section{Introduction}
In this report we will analyze the relative performance of various gradient methods at optimizing a multi-class logistic regression model. Specifically, we are interested in comparing full gradient descent with block coordinate based methods (BCGD) which optimize one coordinate, or column in parameter matrix $X$, at a time. 
Multi-class logistic classification is an extension of traditional logistic regression. The task goes from predicting a Bernoulli random variable to predicting one label to classify each example from a set of $k$ mutually exclusive labels. We encode the target $b_i \in \mathbb{R}^k$ where $k$ is the number of output classes to be a one hot vector, corresponding to the correct classification of example $a_i \in \mathbb{R}^d$. It assigns these labels by learning the conditional probability distribution that the correct label is $b_i$ given example $a_i$ and learned parameter matrix $X \in \mathbb{R}^{d \times k}$ Where $d$ is the number of features in each example. The output distribution is computed with the softmax function in this context: 
$$p(b_i|a_i, X) = \frac{\exp(x_{b_i}^T a_i)}{\sum_{c=1}^k\exp(x_c^Ta_i)}$$
Multi-class logistic regression is a simple yet effective method with ubiquitous use. Its training does, however, require learning the parameter matrix $X$, which can be done using various methods like maximum likelihood or gradient descent based methods. This corresponds to solving the following optimization problem:
$$\min_{x\in\mathbb{R}^{d\times k}} \sum\limits_{i=1}^{m}[-x^T_{b_i}a_{i}+ \log(\sum\limits_{c=1}^k\exp(x_c^{T}a_i))]$$
which is the cross-entropy of the predicted class distribution and the target label $b_i$. Of importance to the application of gradient methods is this problems convexity. This is nice for many reasons, but most importantly we can guarantee that a local minima found by GD methods will be the global minimum.

\section{Algorithms}
Here we provide an overview to the methods studied. We implemented the various methods in python using either pyTorch or Numpy. The three algorithms studied are full gradient descent and two BCGD methods using both Gauss-Southwell and randomized rule.  
\subsection{Gradient Descent}
\begin{algorithm}
\caption{Full Gradient Descent}\label{alg:cap}
\begin{algorithmic}
\State Initialize $X_1 \in \mathbb{R}^{d\times k}$
\For{$k = 1,... $}
    \State If $X \in X^*$ \textbf{STOP} (Optimality conditions)
    \State Set $X_{k+1} = x_k - \alpha_{k}\nabla$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Block coordinate Gradient Descent}
BCGD methods are specialized in optimizing high dimensional problems where where calculation of the gradient can be split up in to blocks which are easier to compute. These methods exploit the convex nature of the problem to optimize one coordinate at a time and approach the global optimum.
\begin{algorithm}
\caption{BCGD}\label{alg:cap}
\begin{algorithmic}
\State Initialize $X_1 \in \mathbb{R}^{d\times k}$
\For{$k = 1,... $}
    \State If $X \in X^*$ \textbf{STOP} (Optimality conditions)
    \State Select $i_k \in $ block according to some \textbf{RULE}
    \State Set $x_{k+1} = x_k - \alpha_k[\nabla_x f(x_k)]_{i_k}$
\EndFor
\end{algorithmic}
\end{algorithm}
Above $\alpha_k \geq 0$ is a stepsize for the $k$th iteration. We later analize the effect of different methods for calculating this. The choice of \textbf{RULE} will distinguish the Gauss-Southwell and Randomized variants of BCGD. The Gauss-Southwell (GS) rule is a greedy selection method that focuses on the block with the largest gradient component at each iteration. This method involves computing the gradient of the objective function for all blocks and selecting the one with the maximum gradient norm. By updating the block that is expected to provide the greatest improvement in the objective function, the GS rule can lead to faster convergence, especially in problems where gradients vary significantly across blocks. However, the computational cost is higher due to the need to evaluate the full gradient at each step, making this approach more complex and potentially less efficient for high-dimensional problems.
For the GS rule we compute 
$$i_k = {\arg\max}_j ||\nabla_j f(x_k)||$$
Taking $i_k$ uniform random samples to be the block index give the randomized rule. This method simplifies the implementation and reduces computational cost, as it only requires computing the gradient of the randomly selected block rather than the full gradient
 

\section{Synthetic Dataset}
To initially very the convergence of our algorithms we create a high-dimensional dataset for the multi-class classification problem. This consists of our data matrix $A\in\mathbb{R}^{1000\times1000}$ and target classes $b$. The rows of $A$ correspond individual examples in the dataset and the columns their numeric features. Entries for $A$ were drawn i.i.d. from a $N(0,1)$ distribution. Here $b\in\mathbb{Z}_{50}$ represents the vector of target classes corresponding to their respective rows in $A$. We define our starting parameter matrix $X\in\mathbb{R}^{1000\times50}$. We can then define a error matrix with standard normal entries to inject noise into our calculation of the target classes
$$b = AX + E$$
and further process $b$ to be the argmax of each row. This dataset has the feature of having an expensive gradient calculation due to the high number of features and output classes. This creates the potential for BCGD methods to be advantaged as ones like randomized rule may compute a smaller block of the gradient.
\subsection{Performance}
\subsection{Stepsize Analysis}
\section{Real-world Dataset}
\end{document}
